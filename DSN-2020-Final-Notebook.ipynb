{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(data):\n",
    "    \n",
    "    '''\n",
    "    Function to input missing values based on the column object type\n",
    "    '''\n",
    "    \n",
    "    cols = list(data.columns)\n",
    "    for col in cols:\n",
    "        if data[col].dtype == 'int64' or data[col].dtype == 'float64':\n",
    "        \n",
    "            data[col] = data[col].fillna(data[col].mean())\n",
    "            \n",
    "        else:\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "            \n",
    "    return data\n",
    " \n",
    "def one_hot_encoding(traindata, *args):\n",
    "    \n",
    "    for ii in args:\n",
    "        traindata = pd.get_dummies(traindata, prefix=[ii], columns=[ii])\n",
    "        \n",
    "    return traindata\n",
    " \n",
    "def drop_columns(traindata, *args):\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    traindata = traindata.drop(columns, axis=1)\n",
    "        \n",
    "    return traindata\n",
    "\n",
    "def make_submission(prediction, filename):\n",
    "    sample = pd.read_csv('SampleSubmission.csv')\n",
    "    test = pd.read_csv('Test.csv')\n",
    "    sample.Applicant_ID = test.Applicant_ID\n",
    "    sample.default_status = prediction\n",
    "    sample.to_csv(filename, index = False)\n",
    " \n",
    "def process(traindata):\n",
    "    \n",
    "    cols = list(traindata.columns)\n",
    "    for _ in cols:\n",
    "        traindata[_] = np.where(traindata[_] == np.inf, -999, traindata[_])\n",
    "        traindata[_] = np.where(traindata[_] == np.nan, -999, traindata[_])\n",
    "        traindata[_] = np.where(traindata[_] == -np.inf, -999, traindata[_])\n",
    "        \n",
    "    return traindata\n",
    " \n",
    "def show_evaluation(pred, true):\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
    " \n",
    "def freq_encode(data, cols):\n",
    "    for i in cols:\n",
    "        encoding = data.groupby(i).size()\n",
    "        encoding = encoding/len(data)\n",
    "        data[i + '_enc'] = data[i].map(encoding)\n",
    "    return data\n",
    " \n",
    " \n",
    "def mean_target(data, cols):\n",
    "    kf = KFold(5)\n",
    "    a = pd.DataFrame()\n",
    "    for tr_ind, val_ind in kf.split(data):\n",
    "        X_tr, X_val= data.iloc[tr_ind].copy(), data.iloc[val_ind].copy()\n",
    "        for col in cols:\n",
    "            means = X_val[col].map(X_tr.groupby(col).default_status.mean())\n",
    "            X_val[col + '_mean_target'] = means + 0.0001\n",
    "        a = pd.concat((a, X_val))\n",
    "    return a\n",
    "        \n",
    "def scale_data(data):\n",
    "  \n",
    "  data = scaler.transform(data)\n",
    "  #testdata = scaler.transform(testdata)\n",
    "  data = pd.DataFrame(data)\n",
    " \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "sample = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.default_status\n",
    "target_numbers = {'yes': 1,\n",
    "                  'no': 0}\n",
    "\n",
    "target = target.map(target_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 52)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "df = pd.concat((train, test)).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80000, 53)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Applicant_ID'], axis=1)\n",
    "\n",
    "df.default_status = target\n",
    "df = freq_encode(df, ['form_field47'])\n",
    "print(df.shape)\n",
    "df = mean_target(df, ['form_field47'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    51840\n",
       "1    28160\n",
       "Name: form_field47_encoded, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['form_field47_encoded'] = df['form_field47'].astype('category')\n",
    "df['form_field47_encoded'] = df['form_field47_encoded'].cat.codes\n",
    "df.form_field47_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING MORE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olawale/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80000, 86)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = ['form_field1', 'form_field2', 'form_field42',\n",
    "                'form_field6']\n",
    "\n",
    "new = pd.DataFrame()\n",
    "for i in feature_cols:\n",
    "    for j in feature_cols:\n",
    "        if i != j:\n",
    "            new[i + '*' + j] = df[i] * df[j]\n",
    "            new[i + '/' + j] = df[i] / df[j]\n",
    "            #new[i + '-' + j] = df[i] - df[j]\n",
    "            new[i + '*' + j] = df[i] + df[j]\n",
    "            \n",
    "    new['_log' + i] = np.log(df[i])\n",
    "    new['_sqrt' + i] = np.sqrt(df[i])\n",
    "    \n",
    "df1 = df.copy()\n",
    "df = pd.concat((df, new), axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 84) (24000, 84)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['form_field47'], axis=1)\n",
    "df.shape\n",
    " \n",
    "df = df.fillna(-999)\n",
    "df = process(df)\n",
    "data = df.copy()\n",
    " \n",
    "train2 = data[:ntrain].copy()\n",
    "train2.drop(['default_status'], axis=1, inplace=True)\n",
    " \n",
    "test2 = data[ntrain:(ntest+ntrain)].copy()\n",
    "test2.drop(['default_status'], axis=1, inplace=True)\n",
    "test2 = test2.reset_index(drop=True)\n",
    " \n",
    "print(train2.shape, test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 94) (24000, 94)\n"
     ]
    }
   ],
   "source": [
    "pca.fit(train2)\n",
    "\n",
    "train3 = pca.transform(train2)\n",
    "train2 = pd.concat((train2, pd.DataFrame(train3)), axis=1)\n",
    "test3 = pca.transform(test2)\n",
    "test2 = pd.concat((test2, pd.DataFrame(test3)), axis=1)\n",
    "print(train2.shape, test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = train2\n",
    "testdata = test2\n",
    " \n",
    "scaler = preprocessing.StandardScaler().fit(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = scale_data(traindata)\n",
    "testdata = scale_data(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.484584\n",
      "[100]\tvalid_0's binary_logloss: 0.45362\n",
      "[150]\tvalid_0's binary_logloss: 0.437704\n",
      "[200]\tvalid_0's binary_logloss: 0.429592\n",
      "[250]\tvalid_0's binary_logloss: 0.425149\n",
      "[300]\tvalid_0's binary_logloss: 0.422397\n",
      "[350]\tvalid_0's binary_logloss: 0.420494\n",
      "[400]\tvalid_0's binary_logloss: 0.418729\n",
      "[450]\tvalid_0's binary_logloss: 0.417655\n",
      "[500]\tvalid_0's binary_logloss: 0.416945\n",
      "[550]\tvalid_0's binary_logloss: 0.416523\n",
      "[600]\tvalid_0's binary_logloss: 0.416067\n",
      "[650]\tvalid_0's binary_logloss: 0.415967\n",
      "[700]\tvalid_0's binary_logloss: 0.415772\n",
      "[750]\tvalid_0's binary_logloss: 0.41542\n",
      "[800]\tvalid_0's binary_logloss: 0.415422\n",
      "[850]\tvalid_0's binary_logloss: 0.415446\n",
      "[900]\tvalid_0's binary_logloss: 0.41553\n",
      "Early stopping, best iteration is:\n",
      "[820]\tvalid_0's binary_logloss: 0.415337\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.478704\n",
      "[100]\tvalid_0's binary_logloss: 0.445656\n",
      "[150]\tvalid_0's binary_logloss: 0.429361\n",
      "[200]\tvalid_0's binary_logloss: 0.42064\n",
      "[250]\tvalid_0's binary_logloss: 0.415622\n",
      "[300]\tvalid_0's binary_logloss: 0.412975\n",
      "[350]\tvalid_0's binary_logloss: 0.411394\n",
      "[400]\tvalid_0's binary_logloss: 0.410276\n",
      "[450]\tvalid_0's binary_logloss: 0.409316\n",
      "[500]\tvalid_0's binary_logloss: 0.408739\n",
      "[550]\tvalid_0's binary_logloss: 0.408623\n",
      "[600]\tvalid_0's binary_logloss: 0.408503\n",
      "[650]\tvalid_0's binary_logloss: 0.408474\n",
      "[700]\tvalid_0's binary_logloss: 0.408481\n",
      "[750]\tvalid_0's binary_logloss: 0.408561\n",
      "Early stopping, best iteration is:\n",
      "[668]\tvalid_0's binary_logloss: 0.408358\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.483971\n",
      "[100]\tvalid_0's binary_logloss: 0.452678\n",
      "[150]\tvalid_0's binary_logloss: 0.437411\n",
      "[200]\tvalid_0's binary_logloss: 0.429818\n",
      "[250]\tvalid_0's binary_logloss: 0.425728\n",
      "[300]\tvalid_0's binary_logloss: 0.423428\n",
      "[350]\tvalid_0's binary_logloss: 0.421836\n",
      "[400]\tvalid_0's binary_logloss: 0.42076\n",
      "[450]\tvalid_0's binary_logloss: 0.42024\n",
      "[500]\tvalid_0's binary_logloss: 0.419642\n",
      "[550]\tvalid_0's binary_logloss: 0.41925\n",
      "[600]\tvalid_0's binary_logloss: 0.418824\n",
      "[650]\tvalid_0's binary_logloss: 0.418346\n",
      "[700]\tvalid_0's binary_logloss: 0.418115\n",
      "[750]\tvalid_0's binary_logloss: 0.417881\n",
      "[800]\tvalid_0's binary_logloss: 0.417681\n",
      "[850]\tvalid_0's binary_logloss: 0.417648\n",
      "[900]\tvalid_0's binary_logloss: 0.417384\n",
      "[950]\tvalid_0's binary_logloss: 0.417583\n",
      "Early stopping, best iteration is:\n",
      "[892]\tvalid_0's binary_logloss: 0.417367\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.480238\n",
      "[100]\tvalid_0's binary_logloss: 0.447226\n",
      "[150]\tvalid_0's binary_logloss: 0.430666\n",
      "[200]\tvalid_0's binary_logloss: 0.421849\n",
      "[250]\tvalid_0's binary_logloss: 0.416887\n",
      "[300]\tvalid_0's binary_logloss: 0.414012\n",
      "[350]\tvalid_0's binary_logloss: 0.41212\n",
      "[400]\tvalid_0's binary_logloss: 0.410856\n",
      "[450]\tvalid_0's binary_logloss: 0.409642\n",
      "[500]\tvalid_0's binary_logloss: 0.408941\n",
      "[550]\tvalid_0's binary_logloss: 0.408358\n",
      "[600]\tvalid_0's binary_logloss: 0.40794\n",
      "[650]\tvalid_0's binary_logloss: 0.407818\n",
      "[700]\tvalid_0's binary_logloss: 0.407692\n",
      "[750]\tvalid_0's binary_logloss: 0.407714\n",
      "[800]\tvalid_0's binary_logloss: 0.407686\n",
      "[850]\tvalid_0's binary_logloss: 0.407581\n",
      "[900]\tvalid_0's binary_logloss: 0.407422\n",
      "[950]\tvalid_0's binary_logloss: 0.407412\n",
      "[1000]\tvalid_0's binary_logloss: 0.407567\n",
      "Early stopping, best iteration is:\n",
      "[930]\tvalid_0's binary_logloss: 0.407353\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.473873\n",
      "[100]\tvalid_0's binary_logloss: 0.437238\n",
      "[150]\tvalid_0's binary_logloss: 0.41784\n",
      "[200]\tvalid_0's binary_logloss: 0.407371\n",
      "[250]\tvalid_0's binary_logloss: 0.4012\n",
      "[300]\tvalid_0's binary_logloss: 0.397461\n",
      "[350]\tvalid_0's binary_logloss: 0.394894\n",
      "[400]\tvalid_0's binary_logloss: 0.39333\n",
      "[450]\tvalid_0's binary_logloss: 0.392151\n",
      "[500]\tvalid_0's binary_logloss: 0.391197\n",
      "[550]\tvalid_0's binary_logloss: 0.390569\n",
      "[600]\tvalid_0's binary_logloss: 0.390101\n",
      "[650]\tvalid_0's binary_logloss: 0.389935\n",
      "[700]\tvalid_0's binary_logloss: 0.38964\n",
      "[750]\tvalid_0's binary_logloss: 0.389547\n",
      "[800]\tvalid_0's binary_logloss: 0.389531\n",
      "[850]\tvalid_0's binary_logloss: 0.389442\n",
      "[900]\tvalid_0's binary_logloss: 0.389462\n",
      "[950]\tvalid_0's binary_logloss: 0.389442\n",
      "Early stopping, best iteration is:\n",
      "[880]\tvalid_0's binary_logloss: 0.389312\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.47868\n",
      "[100]\tvalid_0's binary_logloss: 0.444091\n",
      "[150]\tvalid_0's binary_logloss: 0.425866\n",
      "[200]\tvalid_0's binary_logloss: 0.415609\n",
      "[250]\tvalid_0's binary_logloss: 0.410116\n",
      "[300]\tvalid_0's binary_logloss: 0.407108\n",
      "[350]\tvalid_0's binary_logloss: 0.40508\n",
      "[400]\tvalid_0's binary_logloss: 0.403567\n",
      "[450]\tvalid_0's binary_logloss: 0.40279\n",
      "[500]\tvalid_0's binary_logloss: 0.402373\n",
      "[550]\tvalid_0's binary_logloss: 0.402129\n",
      "[600]\tvalid_0's binary_logloss: 0.402088\n",
      "[650]\tvalid_0's binary_logloss: 0.401928\n",
      "[700]\tvalid_0's binary_logloss: 0.401778\n",
      "[750]\tvalid_0's binary_logloss: 0.401615\n",
      "[800]\tvalid_0's binary_logloss: 0.401463\n",
      "[850]\tvalid_0's binary_logloss: 0.40136\n",
      "[900]\tvalid_0's binary_logloss: 0.401243\n",
      "[950]\tvalid_0's binary_logloss: 0.401206\n",
      "[1000]\tvalid_0's binary_logloss: 0.401074\n",
      "[1050]\tvalid_0's binary_logloss: 0.401013\n",
      "[1100]\tvalid_0's binary_logloss: 0.40109\n",
      "Early stopping, best iteration is:\n",
      "[1035]\tvalid_0's binary_logloss: 0.400993\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.481976\n",
      "[100]\tvalid_0's binary_logloss: 0.450123\n",
      "[150]\tvalid_0's binary_logloss: 0.433806\n",
      "[200]\tvalid_0's binary_logloss: 0.425768\n",
      "[250]\tvalid_0's binary_logloss: 0.420788\n",
      "[300]\tvalid_0's binary_logloss: 0.418034\n",
      "[350]\tvalid_0's binary_logloss: 0.416342\n",
      "[400]\tvalid_0's binary_logloss: 0.415215\n",
      "[450]\tvalid_0's binary_logloss: 0.414435\n",
      "[500]\tvalid_0's binary_logloss: 0.413993\n",
      "[550]\tvalid_0's binary_logloss: 0.41367\n",
      "[600]\tvalid_0's binary_logloss: 0.413489\n",
      "[650]\tvalid_0's binary_logloss: 0.4134\n",
      "[700]\tvalid_0's binary_logloss: 0.41346\n",
      "Early stopping, best iteration is:\n",
      "[634]\tvalid_0's binary_logloss: 0.413315\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.477944\n",
      "[100]\tvalid_0's binary_logloss: 0.444115\n",
      "[150]\tvalid_0's binary_logloss: 0.42665\n",
      "[200]\tvalid_0's binary_logloss: 0.416455\n",
      "[250]\tvalid_0's binary_logloss: 0.410618\n",
      "[300]\tvalid_0's binary_logloss: 0.407015\n",
      "[350]\tvalid_0's binary_logloss: 0.404752\n",
      "[400]\tvalid_0's binary_logloss: 0.40324\n",
      "[450]\tvalid_0's binary_logloss: 0.402608\n",
      "[500]\tvalid_0's binary_logloss: 0.401849\n",
      "[550]\tvalid_0's binary_logloss: 0.401089\n",
      "[600]\tvalid_0's binary_logloss: 0.400528\n",
      "[650]\tvalid_0's binary_logloss: 0.400112\n",
      "[700]\tvalid_0's binary_logloss: 0.399805\n",
      "[750]\tvalid_0's binary_logloss: 0.399309\n",
      "[800]\tvalid_0's binary_logloss: 0.399275\n",
      "[850]\tvalid_0's binary_logloss: 0.399278\n",
      "[900]\tvalid_0's binary_logloss: 0.399329\n",
      "Early stopping, best iteration is:\n",
      "[838]\tvalid_0's binary_logloss: 0.399185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.477745\n",
      "[100]\tvalid_0's binary_logloss: 0.443606\n",
      "[150]\tvalid_0's binary_logloss: 0.426417\n",
      "[200]\tvalid_0's binary_logloss: 0.416967\n",
      "[250]\tvalid_0's binary_logloss: 0.411786\n",
      "[300]\tvalid_0's binary_logloss: 0.408733\n",
      "[350]\tvalid_0's binary_logloss: 0.407205\n",
      "[400]\tvalid_0's binary_logloss: 0.406022\n",
      "[450]\tvalid_0's binary_logloss: 0.405723\n",
      "[500]\tvalid_0's binary_logloss: 0.405667\n",
      "[550]\tvalid_0's binary_logloss: 0.405475\n",
      "[600]\tvalid_0's binary_logloss: 0.405468\n",
      "[650]\tvalid_0's binary_logloss: 0.405549\n",
      "Early stopping, best iteration is:\n",
      "[578]\tvalid_0's binary_logloss: 0.405365\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.480495\n",
      "[100]\tvalid_0's binary_logloss: 0.446881\n",
      "[150]\tvalid_0's binary_logloss: 0.42984\n",
      "[200]\tvalid_0's binary_logloss: 0.420921\n",
      "[250]\tvalid_0's binary_logloss: 0.416079\n",
      "[300]\tvalid_0's binary_logloss: 0.413279\n",
      "[350]\tvalid_0's binary_logloss: 0.411737\n",
      "[400]\tvalid_0's binary_logloss: 0.41075\n",
      "[450]\tvalid_0's binary_logloss: 0.410298\n",
      "[500]\tvalid_0's binary_logloss: 0.410334\n",
      "[550]\tvalid_0's binary_logloss: 0.410221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\tvalid_0's binary_logloss: 0.410077\n",
      "[650]\tvalid_0's binary_logloss: 0.410163\n",
      "[700]\tvalid_0's binary_logloss: 0.410203\n",
      "Early stopping, best iteration is:\n",
      "[618]\tvalid_0's binary_logloss: 0.410023\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.479043\n",
      "[100]\tvalid_0's binary_logloss: 0.444811\n",
      "[150]\tvalid_0's binary_logloss: 0.426305\n",
      "[200]\tvalid_0's binary_logloss: 0.415749\n",
      "[250]\tvalid_0's binary_logloss: 0.410338\n",
      "[300]\tvalid_0's binary_logloss: 0.406555\n",
      "[350]\tvalid_0's binary_logloss: 0.404094\n",
      "[400]\tvalid_0's binary_logloss: 0.402776\n",
      "[450]\tvalid_0's binary_logloss: 0.401517\n",
      "[500]\tvalid_0's binary_logloss: 0.400708\n",
      "[550]\tvalid_0's binary_logloss: 0.400001\n",
      "[600]\tvalid_0's binary_logloss: 0.399711\n",
      "[650]\tvalid_0's binary_logloss: 0.399484\n",
      "[700]\tvalid_0's binary_logloss: 0.399141\n",
      "[750]\tvalid_0's binary_logloss: 0.399004\n",
      "[800]\tvalid_0's binary_logloss: 0.398817\n",
      "[850]\tvalid_0's binary_logloss: 0.398554\n",
      "[900]\tvalid_0's binary_logloss: 0.398313\n",
      "[950]\tvalid_0's binary_logloss: 0.398274\n",
      "[1000]\tvalid_0's binary_logloss: 0.39799\n",
      "[1050]\tvalid_0's binary_logloss: 0.397984\n",
      "[1100]\tvalid_0's binary_logloss: 0.398068\n",
      "Early stopping, best iteration is:\n",
      "[1037]\tvalid_0's binary_logloss: 0.397917\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.479129\n",
      "[100]\tvalid_0's binary_logloss: 0.445931\n",
      "[150]\tvalid_0's binary_logloss: 0.428893\n",
      "[200]\tvalid_0's binary_logloss: 0.419885\n",
      "[250]\tvalid_0's binary_logloss: 0.414532\n",
      "[300]\tvalid_0's binary_logloss: 0.411355\n",
      "[350]\tvalid_0's binary_logloss: 0.409031\n",
      "[400]\tvalid_0's binary_logloss: 0.407536\n",
      "[450]\tvalid_0's binary_logloss: 0.406605\n",
      "[500]\tvalid_0's binary_logloss: 0.405984\n",
      "[550]\tvalid_0's binary_logloss: 0.40574\n",
      "[600]\tvalid_0's binary_logloss: 0.405423\n",
      "[650]\tvalid_0's binary_logloss: 0.405184\n",
      "[700]\tvalid_0's binary_logloss: 0.40525\n",
      "[750]\tvalid_0's binary_logloss: 0.405078\n",
      "[800]\tvalid_0's binary_logloss: 0.405071\n",
      "Early stopping, best iteration is:\n",
      "[733]\tvalid_0's binary_logloss: 0.405047\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.480537\n",
      "[100]\tvalid_0's binary_logloss: 0.447582\n",
      "[150]\tvalid_0's binary_logloss: 0.431143\n",
      "[200]\tvalid_0's binary_logloss: 0.422132\n",
      "[250]\tvalid_0's binary_logloss: 0.417391\n",
      "[300]\tvalid_0's binary_logloss: 0.414603\n",
      "[350]\tvalid_0's binary_logloss: 0.412689\n",
      "[400]\tvalid_0's binary_logloss: 0.411045\n",
      "[450]\tvalid_0's binary_logloss: 0.409858\n",
      "[500]\tvalid_0's binary_logloss: 0.409687\n",
      "[550]\tvalid_0's binary_logloss: 0.409542\n",
      "[600]\tvalid_0's binary_logloss: 0.409531\n",
      "[650]\tvalid_0's binary_logloss: 0.409379\n",
      "[700]\tvalid_0's binary_logloss: 0.409119\n",
      "[750]\tvalid_0's binary_logloss: 0.409127\n",
      "[800]\tvalid_0's binary_logloss: 0.408905\n",
      "[850]\tvalid_0's binary_logloss: 0.408943\n",
      "[900]\tvalid_0's binary_logloss: 0.408907\n",
      "[950]\tvalid_0's binary_logloss: 0.408994\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's binary_logloss: 0.408834\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.480533\n",
      "[100]\tvalid_0's binary_logloss: 0.447948\n",
      "[150]\tvalid_0's binary_logloss: 0.431147\n",
      "[200]\tvalid_0's binary_logloss: 0.422175\n",
      "[250]\tvalid_0's binary_logloss: 0.417038\n",
      "[300]\tvalid_0's binary_logloss: 0.414668\n",
      "[350]\tvalid_0's binary_logloss: 0.412709\n",
      "[400]\tvalid_0's binary_logloss: 0.411256\n",
      "[450]\tvalid_0's binary_logloss: 0.409836\n",
      "[500]\tvalid_0's binary_logloss: 0.408761\n",
      "[550]\tvalid_0's binary_logloss: 0.408169\n",
      "[600]\tvalid_0's binary_logloss: 0.407556\n",
      "[650]\tvalid_0's binary_logloss: 0.407132\n",
      "[700]\tvalid_0's binary_logloss: 0.407036\n",
      "[750]\tvalid_0's binary_logloss: 0.406646\n",
      "[800]\tvalid_0's binary_logloss: 0.406558\n",
      "[850]\tvalid_0's binary_logloss: 0.40605\n",
      "[900]\tvalid_0's binary_logloss: 0.405973\n",
      "[950]\tvalid_0's binary_logloss: 0.405827\n",
      "[1000]\tvalid_0's binary_logloss: 0.405607\n",
      "[1050]\tvalid_0's binary_logloss: 0.405667\n",
      "[1100]\tvalid_0's binary_logloss: 0.405722\n",
      "Early stopping, best iteration is:\n",
      "[1021]\tvalid_0's binary_logloss: 0.405556\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.483064\n",
      "[100]\tvalid_0's binary_logloss: 0.450561\n",
      "[150]\tvalid_0's binary_logloss: 0.433795\n",
      "[200]\tvalid_0's binary_logloss: 0.425267\n",
      "[250]\tvalid_0's binary_logloss: 0.420422\n",
      "[300]\tvalid_0's binary_logloss: 0.417926\n",
      "[350]\tvalid_0's binary_logloss: 0.416295\n",
      "[400]\tvalid_0's binary_logloss: 0.415607\n",
      "[450]\tvalid_0's binary_logloss: 0.415236\n",
      "[500]\tvalid_0's binary_logloss: 0.415191\n",
      "[550]\tvalid_0's binary_logloss: 0.415018\n",
      "[600]\tvalid_0's binary_logloss: 0.41485\n",
      "[650]\tvalid_0's binary_logloss: 0.414963\n",
      "[700]\tvalid_0's binary_logloss: 0.415133\n",
      "Early stopping, best iteration is:\n",
      "[611]\tvalid_0's binary_logloss: 0.414803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.47834\n",
      "[100]\tvalid_0's binary_logloss: 0.443988\n",
      "[150]\tvalid_0's binary_logloss: 0.425991\n",
      "[200]\tvalid_0's binary_logloss: 0.416327\n",
      "[250]\tvalid_0's binary_logloss: 0.410946\n",
      "[300]\tvalid_0's binary_logloss: 0.407879\n",
      "[350]\tvalid_0's binary_logloss: 0.405862\n",
      "[400]\tvalid_0's binary_logloss: 0.404495\n",
      "[450]\tvalid_0's binary_logloss: 0.40333\n",
      "[500]\tvalid_0's binary_logloss: 0.402441\n",
      "[550]\tvalid_0's binary_logloss: 0.401714\n",
      "[600]\tvalid_0's binary_logloss: 0.401207\n",
      "[650]\tvalid_0's binary_logloss: 0.400898\n",
      "[700]\tvalid_0's binary_logloss: 0.400626\n",
      "[750]\tvalid_0's binary_logloss: 0.400535\n",
      "[800]\tvalid_0's binary_logloss: 0.400578\n",
      "[850]\tvalid_0's binary_logloss: 0.400551\n",
      "Early stopping, best iteration is:\n",
      "[787]\tvalid_0's binary_logloss: 0.400521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.479947\n",
      "[100]\tvalid_0's binary_logloss: 0.446886\n",
      "[150]\tvalid_0's binary_logloss: 0.429925\n",
      "[200]\tvalid_0's binary_logloss: 0.421138\n",
      "[250]\tvalid_0's binary_logloss: 0.41598\n",
      "[300]\tvalid_0's binary_logloss: 0.413265\n",
      "[350]\tvalid_0's binary_logloss: 0.411746\n",
      "[400]\tvalid_0's binary_logloss: 0.410624\n",
      "[450]\tvalid_0's binary_logloss: 0.409824\n",
      "[500]\tvalid_0's binary_logloss: 0.409138\n",
      "[550]\tvalid_0's binary_logloss: 0.408916\n",
      "[600]\tvalid_0's binary_logloss: 0.409047\n",
      "Early stopping, best iteration is:\n",
      "[538]\tvalid_0's binary_logloss: 0.408897\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.480887\n",
      "[100]\tvalid_0's binary_logloss: 0.448107\n",
      "[150]\tvalid_0's binary_logloss: 0.430828\n",
      "[200]\tvalid_0's binary_logloss: 0.422242\n",
      "[250]\tvalid_0's binary_logloss: 0.417204\n",
      "[300]\tvalid_0's binary_logloss: 0.414009\n",
      "[350]\tvalid_0's binary_logloss: 0.412181\n",
      "[400]\tvalid_0's binary_logloss: 0.411342\n",
      "[450]\tvalid_0's binary_logloss: 0.410818\n",
      "[500]\tvalid_0's binary_logloss: 0.410272\n",
      "[550]\tvalid_0's binary_logloss: 0.409916\n",
      "[600]\tvalid_0's binary_logloss: 0.409637\n",
      "[650]\tvalid_0's binary_logloss: 0.409453\n",
      "[700]\tvalid_0's binary_logloss: 0.409457\n",
      "Early stopping, best iteration is:\n",
      "[639]\tvalid_0's binary_logloss: 0.409331\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.479207\n",
      "[100]\tvalid_0's binary_logloss: 0.446654\n",
      "[150]\tvalid_0's binary_logloss: 0.429758\n",
      "[200]\tvalid_0's binary_logloss: 0.420659\n",
      "[250]\tvalid_0's binary_logloss: 0.415945\n",
      "[300]\tvalid_0's binary_logloss: 0.413044\n",
      "[350]\tvalid_0's binary_logloss: 0.411459\n",
      "[400]\tvalid_0's binary_logloss: 0.410462\n",
      "[450]\tvalid_0's binary_logloss: 0.40975\n",
      "[500]\tvalid_0's binary_logloss: 0.409552\n",
      "[550]\tvalid_0's binary_logloss: 0.409533\n",
      "[600]\tvalid_0's binary_logloss: 0.409676\n",
      "Early stopping, best iteration is:\n",
      "[542]\tvalid_0's binary_logloss: 0.409484\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.483612\n",
      "[100]\tvalid_0's binary_logloss: 0.451763\n",
      "[150]\tvalid_0's binary_logloss: 0.435458\n",
      "[200]\tvalid_0's binary_logloss: 0.426705\n",
      "[250]\tvalid_0's binary_logloss: 0.422013\n",
      "[300]\tvalid_0's binary_logloss: 0.419045\n",
      "[350]\tvalid_0's binary_logloss: 0.416862\n",
      "[400]\tvalid_0's binary_logloss: 0.415416\n",
      "[450]\tvalid_0's binary_logloss: 0.414512\n",
      "[500]\tvalid_0's binary_logloss: 0.413837\n",
      "[550]\tvalid_0's binary_logloss: 0.413444\n",
      "[600]\tvalid_0's binary_logloss: 0.413021\n",
      "[650]\tvalid_0's binary_logloss: 0.412836\n",
      "[700]\tvalid_0's binary_logloss: 0.412719\n",
      "[750]\tvalid_0's binary_logloss: 0.412586\n",
      "[800]\tvalid_0's binary_logloss: 0.412536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\tvalid_0's binary_logloss: 0.412381\n",
      "[900]\tvalid_0's binary_logloss: 0.412472\n",
      "[950]\tvalid_0's binary_logloss: 0.412506\n",
      "[1000]\tvalid_0's binary_logloss: 0.412506\n",
      "Early stopping, best iteration is:\n",
      "[934]\tvalid_0's binary_logloss: 0.412327\n"
     ]
    }
   ],
   "source": [
    "nsplit=20\n",
    "\n",
    "kf = StratifiedKFold(n_splits=nsplit, shuffle=True)\n",
    "\n",
    "lgbm = CatBoostClassifier(n_estimators=50000, max_depth=8, \n",
    "                          random_state=14, learning_rate=0.033, \n",
    "                          use_best_model=True, task_type='CPU', \n",
    "                          eval_metric='AUC')\n",
    "\n",
    "lgbm = XGBClassifier(n_estimators=100000, max_depth=8, \n",
    "                     booster='gbtree', base_score=0.7,\n",
    "                     learning_rate=0.033, reg_lambda=30,\n",
    "                     subsample=0.9, colsample_bytree=0.9,\n",
    "                     eval_metric='auc', random_state=20920)\n",
    "\n",
    "lgbm = LGBMClassifier(max_depth=8, num_leaves=64, boosting_type='gbdt', \n",
    "                      learning_rate=0.01, n_estimators=50000, subsample=0.9, \n",
    "                      eval_metric='auc', colsample_bytree=0.9, random_state=2)\n",
    "            \n",
    "pred_test2 = np.zeros((len(test2), 2))\n",
    "\n",
    "for (train_index,test_index) in kf.split(pd.DataFrame(traindata), target):\n",
    "    \n",
    "    X_train,X_test = pd.DataFrame(traindata).iloc[train_index], pd.DataFrame(traindata).iloc[test_index]\n",
    "    y_train,y_test = train.default_status.iloc[train_index],train.default_status.iloc[test_index]\n",
    "    lgbm.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_test, y_test)], verbose=50)\n",
    "    pred_test2 += lgbm.predict_proba(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29167586, 0.44113823, 0.37187728, 0.72132975, 0.12927763,\n",
       "       0.37456067, 0.36216038, 0.57949308, 0.44301859, 0.28477819,\n",
       "       0.12845561, 0.03396455, 0.61630094, 0.03263858, 0.44867898,\n",
       "       0.62573149, 0.29946163, 0.0420398 , 0.09430652, 0.72916864])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_avg = pred_test2/nsplit\n",
    "pred_test2 = pred_test_avg[:,1]\n",
    "pred_test2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(pred_test2, '20lgbfold-8max-depth1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result was gotten from the blending and ensemble of five different models;\n",
    "    -One LightGBM\n",
    "    -One Xgboost\n",
    "    -Three catboost predictions\n",
    "    \n",
    "All models were trained with the hyperparameters above and features generated apart from the other two catboost\n",
    "models that were trained with different amount of features. One of them used only the features provided (as the baseline), while the other used 50 more features generated using PCA from sklearn.decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the predictions were blended based on their performance on the LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = pd.read_csv('20xgbfold-8max-depth1.csv')  #0.84323\n",
    "lg = pd.read_csv('LGB_all_features.csv')  #0.8428\n",
    "cat0 = pd.read_csv('cat_80features.csv')   #0.84393\n",
    "cat1 = pd.read_csv('20catfold-8max-depth2.csv')  #0.84442\n",
    "cat2 = pd.read_csv('10catfold-8max-depth2.csv')   #0.84441\n",
    "\n",
    "b9 = (lg.default_status*0.1 + xg.default_status*0.2 +  cat0.default_status*0.1 +  \n",
    "      cat1.default_status*0.3 + cat2.default_status * 0.3)\n",
    "\n",
    "make_submission(b9, 'b9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission b9 was the best result on the LB which I could reproduce with a score of 0.844741368813585.\n",
    "However my final submission had a score of 0.844743419216839 butI could not find the right predictions I blended to get that as my Jupyter notebook wasn't saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
